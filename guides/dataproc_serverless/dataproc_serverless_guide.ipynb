{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c058d99",
   "metadata": {},
   "source": [
    "# Dataproc Serverless - Tabular Setup Guide\n",
    "Hey, welcome!\n",
    "\n",
    "A few prereqs:\n",
    "- you'll need the google CLI installed\n",
    "- you'll need pipenv installed if you want to use the `make lab` target to fire up your jupyter lab instance\n",
    "\n",
    "## Deploying your custom image\n",
    "Dataproc insists on custom docker images in the GCP artifact registry. I have a Makefile that handles all the dirty work for that below.\n",
    "\n",
    "First, clone this repository and from this directory run the following:\n",
    "```bash\n",
    "make push\n",
    "```\n",
    "\n",
    "This command:\n",
    "- builds a docker image that installs dataproc serverless prereqs and installs the iceberg dependencies for spark\n",
    "- tags the image with your current project id and the region in the makefile (modify these if you like)\n",
    "- authenticates with google\n",
    "- creates an artifact repository to push to (if this fails because it isn't enabled, jump into GCP and enable this API)\n",
    "- pushes the image to the google artifact repository \n",
    "- **Note:** you'll want to go to GCP IAM and grant the Artifact Repository Read role to your compute service account. The default compute service account should be fine unless you select a specific service account to execute as, in which case it will need access. Otherwise the dataproc init will fail when trying to pull this image.\n",
    "\n",
    "## Fire up jupyter lab\n",
    "Google has a jupyter lab plugin for interactive spark jobs on serverless dataproc.\n",
    "\n",
    "Make sure you have pipenv installed (`pip install pipenv`) and run the following:\n",
    "```\n",
    "make lab\n",
    "```\n",
    "\n",
    "This installs the dependencies from the Pipefile and runs your jupyter lab server\n",
    "\n",
    "## Create a serverless template\n",
    "Once you open the jupyter lab UI, do the following:\n",
    "- open a new jupyter tab, select create serverless template in the dataproc section\n",
    "- give your template a name, copy and paste your Current image ID from the output of the `make push` command in the custom image area.\n",
    "- select your network\n",
    "- **Important:** for your network, you need to make sure all internal communication is enabled within the network so your spark instances can communicate correctly and enable the private network access. [See bullet three in this GCP docs section for more info](https://cloud.google.com/dataproc-serverless/docs/quickstarts/jupyterlab-sessions)\n",
    "- no need to save any spark configs up front -- all iceberg configs are provided in the pyspark below. \n",
    "- hit save, and you're good to go\n",
    "\n",
    "## Start Kernel and Test\n",
    "After hitting save, you should be able to open this file in jupyter lab and select your template as your kernel.\n",
    "\n",
    "You may need to refresh the page if you don't see the kernel as an option right away.\n",
    "\n",
    "After that, fill in your own `tabular_credential` and `warehouse_name` below and you're good to test connectivity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcee8a6-aa4b-42f3-8345-1d5509033328",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ‘‡ replace this with your own credential and your tabular warehouse name\n",
    "tabular_credential = 't-123-123'\n",
    "warehouse_name = 'rpw_gcp'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pkgs_str = 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12-1.4.3,org.apache.iceberg:iceberg-gcp-bundle-1.4.3'\n",
    "\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "    .appName(\"Iceberg\")\n",
    "    .config(\"spark.jars.packages\", pkgs_str)\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(f\"spark.sql.catalog.{warehouse_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{warehouse_name}.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{warehouse_name}.uri\", \"https://api.tabular.io/ws\")\n",
    "    .config(f\"spark.sql.catalog.{warehouse_name}.credential\", tabular_credential)\n",
    "    .config(f\"spark.sql.catalog.{warehouse_name}.warehouse\", warehouse_name)\n",
    "    .config(\"spark.sql.defaultCatalog\", warehouse_name)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sql(f'SHOW CATALOGS;').show()\n",
    "\n",
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {warehouse_name}.DATAPROC_INIT;')\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {warehouse_name}.DATAPROC_INIT.HELLO_WORLD AS (SELECT 1 AS ID);')\n",
    "\n",
    "\n",
    "spark.sql(\n",
    "    f'SELECT * FROM {warehouse_name}.DATAPROC_INIT.HELLO_WORLD;'\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tabular 2 on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-tabular_2"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
