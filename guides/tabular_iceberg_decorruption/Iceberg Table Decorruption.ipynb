{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63f07af",
   "metadata": {},
   "source": [
    "# De-corrupt these tables!!\n",
    "- There are bad data files (either missing in s3 OR with odd permissions) in Iceberg table metadata\n",
    "- this script identifies the bad files across an entire warehouse and removes them with iceberg deletes\n",
    "- these deletes can be rolled back if necessary just like any other iceberg operation.\n",
    "\n",
    "\n",
    "The hope is that with these corrupted data files gone, we can proceed with deduplication of tables to fully fix them forever.\n",
    "\n",
    "## Scope\n",
    "Fixing includes 3 steps. \n",
    "\n",
    "Step one is to decorrupt the catalogs:\n",
    "- my_warehouse -- ‚úÖ fixed, had many corruptions \n",
    "\n",
    "Step 2 is to deduplicate the tables in the catalogs.\n",
    "- my_warehouse -- ‚úÖ done!\n",
    "\n",
    "Step 3 is to promote the deduplicated tables.\n",
    "- my_warehouse -- ‚úÖ done!\n",
    "\n",
    "## First, let's configure spark a little better for this EMR instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.app.name\": \"Tabular - Iceberg Table Decorruption\",\n",
    "    \n",
    "    \"spark.master\": \"yarn\",\n",
    "    \"spark.executor.memory\": \"28g\",\n",
    "    \"spark.executor.cores\": \"4\",\n",
    "    \"spark.executor.instances\": \"24\",\n",
    "    \"spark.driver.memory\": \"16g\",\n",
    "    \"spark.driver.cores\": \"4\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "      \n",
    "    \"spark.dynamicAllocation.minExecutors\": \"1\",    \n",
    "    \n",
    "    \"spark.sql.catalog.my_warehouse\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.my_warehouse.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n",
    "    \"spark.sql.catalog.my_warehouse.credential\": \"t-asdf:asdf-asdf\",\n",
    "    \"spark.sql.catalog.my_warehouse.region\": \"us-east-1\",\n",
    "    \"spark.sql.catalog.my_warehouse.uri\": \"https://api.tabular.io/ws/\",\n",
    "    \"spark.sql.catalog.my_warehouse.warehouse\": \"my_warehouse\",\n",
    "      \n",
    "    \"spark.sql.defaultCatalog\": \"my_warehouse\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc18c1",
   "metadata": {},
   "source": [
    "## Next, let's create an iceberg database to contain our work and hold logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b927b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists tabular_support;\")\n",
    "spark.sql(\"\"\"\n",
    "    create table if not exists tabular_support.logs as (\n",
    "        select\n",
    "            current_timestamp() as event_ts,\n",
    "            'id' as batch_id,\n",
    "            'log' as event_type,\n",
    "            'hello' as event_message\n",
    "        limit 0\n",
    "    );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acedfa",
   "metadata": {},
   "source": [
    "## Next, time for some helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ef136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.iceberg.spark.Spark3Util\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "import scala.util.Random\n",
    "\n",
    "def getRandomBatchId(length: Int = 6): String = {\n",
    "  val chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "  (1 to length).map(_ => chars(Random.nextInt(chars.length))).mkString\n",
    "}\n",
    "\n",
    "def log_to_tabular(batch_id: String, msg_type: String, msg: String): Unit = {\n",
    "    println(s\"$batch_id -- $msg\")\n",
    "    spark.sql(s\"\"\"\n",
    "      INSERT INTO tabular_support.logs VALUES \n",
    "          (current_timestamp(), '$batch_id', '$msg_type', '$msg')\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "def getFileCount(spark: SparkSession, tableIdentifier: String): Long = {\n",
    "  spark.table(s\"$tableIdentifier.files\").count()\n",
    "}\n",
    "\n",
    "/**\n",
    "  * This function iterates through all databases except the tabular System database\n",
    "  * to get all tables in the catalog.\n",
    "  *\n",
    "  * - Accepts a spark session, string catalog name, and batch_id for logging\n",
    "  * - Returns a list of table identifiers in the format of \"{database_name}.{table_name}\"\n",
    " **/ \n",
    "def listTablesInCatalog(spark: SparkSession, batch_id: String): List[String] = {\n",
    "  // Get the list of all databases in the specified catalog\n",
    "  val databases = spark.catalog.listDatabases().collect().filter {\n",
    "      db => db.name != \"system\" && db.name != \"tabular_support\" && db.name != \"examples\" && !db.name.endsWith(\"_deduplicated\")\n",
    "  }\n",
    "  \n",
    "  // Filter out the 'system' database and list tables in each remaining database\n",
    "  val tables = databases.flatMap { db =>\n",
    "    log_to_tabular(batch_id, \"decorruption.listTablesInCatalog\", s\"Getting tables for database ${db.name}\")\n",
    "    spark.catalog.listTables(db.name).collect().map { table =>\n",
    "      s\"${db.name}.${table.name}\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  tables.toList\n",
    "}\n",
    "\n",
    "/**\n",
    "  * This function identifies and optionally removes files in a given iceberg table identifier's\n",
    "  * metadata. The files to be removed are files that do not appear to exist anymore.\n",
    "  *\n",
    "  * - Accepts a spark session, string table identifier, and a dry_run flag controlling when to\n",
    "  *      actually delete the missing files or not. When dry_run = true, no deletes occur.\n",
    " **/ \n",
    "def performDeletes(spark: SparkSession, table_identifier: String, dry_run: Boolean = true, batch_id: String): Unit = {    \n",
    "  val curried_log = (log_to_tabular _).curried\n",
    "  val log = curried_log(batch_id)(s\"decorruption.performDeletes.table=$table_identifier\")\n",
    "    \n",
    "  // Create UDF\n",
    "  val iceberg_table = Spark3Util.loadIcebergTable(spark, table_identifier)\n",
    "  val iceberg_table_io = iceberg_table.io\n",
    "  val fileExists = udf((path: String) => {\n",
    "    try {\n",
    "      iceberg_table_io.newInputFile(path).exists()\n",
    "    } catch {\n",
    "      case _: Exception => false\n",
    "    }\n",
    "  })\n",
    "\n",
    "  // Load spark table\n",
    "  val iceberg_files_table = spark.table(s\"$table_identifier.files\").persist()\n",
    "\n",
    "  // Use UDF to get missing files\n",
    "  val missing_files = iceberg_files_table\n",
    "    .select(\"file_path\")\n",
    "    .withColumn(\"exists\", fileExists(col(\"file_path\")))\n",
    "    .select(\"file_path\", \"exists\")\n",
    "    .where(\"exists != true\")\n",
    "\n",
    "\n",
    "  // Perform deletes if dry_run is false\n",
    "  if (!dry_run) {\n",
    "    log(s\"Performing live decorruption of $table_identifier\")\n",
    "      \n",
    "    // Collect missing file paths\n",
    "    val files = missing_files.select(\"file_path\").as[String].collect()\n",
    "    \n",
    "    // Return early if there are no files\n",
    "    if (files.isEmpty) {\n",
    "      log(s\"‚úÖ no missing files found for ${table_identifier}. Moving on üí™\\n\")\n",
    "      return\n",
    "    }\n",
    "      \n",
    "    // delete missing files from iceberg table\n",
    "    var num_retries = 3\n",
    "    while (num_retries > 0) {\n",
    "        try {\n",
    "            var del = Spark3Util.loadIcebergTable(spark, table_identifier).newDelete()\n",
    "            val num_files_to_fix = files.size\n",
    "            var counter = 0\n",
    "            for (file <- files) {\n",
    "              counter += 1\n",
    "              log(s\"\\tüöß fixing file number $counter out of $num_files_to_fix total to fix\")\n",
    "              del = del.deleteFile(file)\n",
    "            }\n",
    "\n",
    "            log(s\"üì¨ all files handled for ${table_identifier}. Sending commit to Tabular\")\n",
    "            del.commit()\n",
    "            log(s\"‚úÖ Commit succeeded for ${table_identifier}. Moving on üí™\\n\")\n",
    "            return\n",
    "        } catch {\n",
    "            case _: Throwable => \n",
    "                num_retries -= 1 \n",
    "        }\n",
    "    }\n",
    "      \n",
    "    // Out of retries\n",
    "    log(s\"‚ùå Commit failed after 3 retries for ${table_identifier}. Moving on üíî\\n\")\n",
    "    \n",
    "  } else {\n",
    "    val num_missing_files = missing_files.count()\n",
    "    log(s\"Table: $table_identifier\\t\\tnum_missing_files:$num_missing_files\")\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def deduplicate_table(spark: SparkSession, table: String, dry_run: Boolean, batch_id: String): Unit = {\n",
    "  val curried_log = (log_to_tabular _).curried\n",
    "  val log = curried_log(batch_id)(s\"deduplicate.table=$table\")\n",
    "  log(s\"\\nStarted deduplication for table $table\")\n",
    "\n",
    "  // Extract database name and table name\n",
    "  val tableParts = table.split(\"\\\\.\")\n",
    "  val databaseName = tableParts(0)\n",
    "  val tableName = tableParts(1)\n",
    "  val deduplicatedDatabaseName = s\"${databaseName}_deduplicated\"\n",
    "\n",
    "  // Check if the deduplicated table already exists\n",
    "  val deduplicatedTableExists = spark.catalog.tableExists(s\"${deduplicatedDatabaseName}.${tableName}\")\n",
    "\n",
    "  if (deduplicatedTableExists) {\n",
    "    log(s\"‚úÖ Skipping deduplication for table $table because deduplicated version already exists\")\n",
    "  } else {\n",
    "    // Create deduplicated database if it doesn't exist\n",
    "    spark.sql(s\"CREATE DATABASE IF NOT EXISTS $deduplicatedDatabaseName\")\n",
    "\n",
    "    // Load the table into a DataFrame\n",
    "    val df = spark.table(table)\n",
    "\n",
    "    // Generate a surrogate key for the entire row by concatenating all columns and applying a hash function\n",
    "    val surrogate_key_col = hash(df.columns.map(c => col(c).cast(\"string\")): _*).alias(\"surrogate_key\")\n",
    "\n",
    "    // Add the surrogate key column to the DataFrame\n",
    "    val df_with_key = df.withColumn(\"surrogate_key\", surrogate_key_col)\n",
    "\n",
    "    // Deduplicate the DataFrame based on the surrogate key\n",
    "    val deduplicated_df = df_with_key.dropDuplicates(\"surrogate_key\").drop(\"surrogate_key\")\n",
    "\n",
    "    if (!dry_run) {\n",
    "      // Write the deduplicated DataFrame to the new database\n",
    "      deduplicated_df.write.mode(SaveMode.Overwrite).saveAsTable(s\"$deduplicatedDatabaseName.$tableName\")\n",
    "      log(s\"Table $table has been deduplicated and saved to $deduplicatedDatabaseName.$tableName\")\n",
    "    }\n",
    "\n",
    "    log(s\"‚úÖ Finished deduplication for table $table\")\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "def archive_database(spark: SparkSession, databaseName: String): String = {\n",
    "  // Check if the database name is in the refused list or ends with `_deduplicated` or `_archived`\n",
    "  if (Set(\"system\", \"examples\", \"tabular_support\").contains(databaseName) || \n",
    "      databaseName.endsWith(\"_deduplicated\") || \n",
    "      databaseName.endsWith(\"_archived\")) {\n",
    "    return s\"Refusing to archive database: $databaseName -- not a valid database for archival\"\n",
    "  }\n",
    "\n",
    "  // Define the archived database name\n",
    "  val archivedDatabaseName = s\"${databaseName}_archived\"\n",
    "\n",
    "  // Create the archived database if it doesn't exist\n",
    "  spark.sql(s\"CREATE DATABASE IF NOT EXISTS $archivedDatabaseName\")\n",
    "\n",
    "  // Get the list of tables in the original database\n",
    "  val tables = spark.catalog.listTables(databaseName).collect()\n",
    "\n",
    "  // Move each table to the archived database\n",
    "  tables.foreach { table =>\n",
    "    val tableName = table.name\n",
    "    spark.sql(s\"ALTER TABLE $databaseName.$tableName RENAME TO $archivedDatabaseName.$tableName\")\n",
    "  }\n",
    "    \n",
    "  return s\"Successfully archived $databaseName to $archivedDatabaseName\"\n",
    "}\n",
    "\n",
    "def promote_deduplicated_database(spark: SparkSession, databaseName: String): String = {\n",
    "  // Check if the database name ends with `_deduplicated`\n",
    "  if (!databaseName.endsWith(\"_deduplicated\")) {\n",
    "    return s\"‚ùå Refusing to promote database: $databaseName -- only *_deduplicated databases can be promoted\"\n",
    "  }\n",
    "\n",
    "  // Define the target database name by removing `_deduplicated`\n",
    "  val targetDatabaseName = databaseName.stripSuffix(\"_deduplicated\")\n",
    "\n",
    "  // Check if the target database exists\n",
    "  val targetDatabaseExists = spark.catalog.databaseExists(targetDatabaseName)\n",
    "  if (!targetDatabaseExists) {\n",
    "    return s\"‚ùå Target database $targetDatabaseName does not exist\"\n",
    "  }\n",
    "\n",
    "  // Check if the target database is empty\n",
    "  val live_tables = spark.catalog.listTables(targetDatabaseName).collect()\n",
    "  if (live_tables.nonEmpty) {\n",
    "    return s\"‚ùå Target database $targetDatabaseName is not empty! Found tables $live_tables\"\n",
    "  }\n",
    "\n",
    "  // Get the list of tables in the deduplicated database\n",
    "  val tables = spark.catalog.listTables(databaseName).collect()\n",
    "\n",
    "  // Move each table to the target database\n",
    "  tables.foreach { table =>\n",
    "    val tableName = table.name\n",
    "    spark.sql(s\"ALTER TABLE $databaseName.$tableName RENAME TO $targetDatabaseName.$tableName\")\n",
    "  }\n",
    "\n",
    "  s\"Successfully promoted $databaseName to $targetDatabaseName\"\n",
    "}\n",
    "\n",
    "def dropDatabase(spark: SparkSession, databaseName: String): Unit = {\n",
    "  // Get the list of tables in the database\n",
    "  val tables = spark.catalog.listTables(databaseName).collect()\n",
    "\n",
    "  // Drop each table in the database\n",
    "  tables.foreach { table =>\n",
    "    val tableName = table.name\n",
    "    try {\n",
    "      spark.sql(s\"DROP TABLE IF EXISTS $databaseName.$tableName\")\n",
    "      println(s\"Successfully dropped table: $tableName\")\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        println(s\"Error dropping table: $tableName - ${e.getMessage}\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Drop the database\n",
    "  try {\n",
    "    spark.sql(s\"DROP DATABASE IF EXISTS $databaseName\")\n",
    "    println(s\"Successfully dropped database: $databaseName\")\n",
    "  } catch {\n",
    "    case e: Exception =>\n",
    "      println(s\"Error dropping database: $databaseName - ${e.getMessage}\")\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "def cleanup_deduplicated_databases(): Unit = {\n",
    "    val databases = spark.sql(\"SHOW DATABASES\").collect().map(_.getString(0))\n",
    "\n",
    "    val deduplicatedDatabases = databases.filter(db => db.endsWith(\"_deduplicated\"))\n",
    "\n",
    "    deduplicatedDatabases.foreach { db =>\n",
    "      try {\n",
    "        // Drop the database\n",
    "        spark.sql(s\"DROP DATABASE IF EXISTS $db\")\n",
    "        println(s\"‚úÖ Successfully dropped database $db\")\n",
    "          \n",
    "      } catch {\n",
    "        case e: Exception =>\n",
    "          println(s\"‚ùå Error dropping database $db: ${e.getMessage}\")\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def archive_and_promote_catalog(spark: SparkSession): Unit = {\n",
    "    val current_catalog = spark.catalog.currentCatalog\n",
    "    val batch_id = getRandomBatchId() + s\" -- $current_catalog\"\n",
    "    \n",
    "    val curried_log = (log_to_tabular _).curried\n",
    "    val log = curried_log(batch_id)(s\"archive_and_promote_catalog\")\n",
    "    log(s\"Started batch $batch_id \")\n",
    "    \n",
    "    // get databases to work with\n",
    "    val databases = spark.sql(\"SHOW DATABASES\").collect().map(_.getString(0)).filter {\n",
    "            db => !db.startsWith(\"bronze_metadata\")\n",
    "    }\n",
    "    val deduplicated_databases = databases.filter(db => db.endsWith(\"_deduplicated\"))\n",
    "    val live_databases = databases.filter { db => \n",
    "        !db.endsWith(\"_deduplicated\") && \n",
    "        !db.endsWith(\"_archived\") && \n",
    "        db != \"system\" && \n",
    "        db != \"examples\" && \n",
    "        db != \"tabular_support\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    // archive the live tables\n",
    "    live_databases.foreach { live_db =>\n",
    "        try {\n",
    "            log(archive_database(spark, live_db) + \"\\n\")\n",
    "        } catch {\n",
    "          case e: Exception =>\n",
    "            // The typical exceptions have full stack traces in the error message, so grab just the first line of the message\n",
    "            log(s\"‚ùå Exception during archival for database $live_db: ${e.getMessage.split(\"\\n\").head}\" + \"\\n\")\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    \n",
    "    log(s\"\\n\\nüíé All live databases have been archived! Moving on to promote deduplicated databases\\n\\n\")\n",
    "    \n",
    "    \n",
    "    // promote the deduplicated tables\n",
    "    deduplicated_databases.foreach { dedup_db =>\n",
    "        try {\n",
    "            log(promote_deduplicated_database(spark, dedup_db))\n",
    "        } catch {\n",
    "          case e: Exception =>\n",
    "            // The typical exceptions have full stack traces in the error message, so grab just the first line of the message\n",
    "            log(s\"‚ùå Exception during archival for database $dedup_db: ${e.getMessage.split(\"\\n\").head}\")\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    \n",
    "    // cleanup empty deduplicated databases\n",
    "    log(s\"\\nüí™ Removing all empty deduplicated databases : )\")\n",
    "    cleanup_deduplicated_databases()\n",
    "    \n",
    "    log(s\"\\n‚úÖ Completed batch $batch_id\")\n",
    "}\n",
    "\n",
    "\n",
    "def decorrupt_catalog(spark: SparkSession, dry_run: Boolean = true): Unit = {\n",
    "  val current_catalog = spark.catalog.currentCatalog\n",
    "  val batch_id = getRandomBatchId() + s\" -- $current_catalog\"\n",
    "  log_to_tabular(batch_id, s\"decorruption.start\", s\"Started batch $batch_id\")\n",
    "    \n",
    "  // List all tables in the catalog\n",
    "  val tables = listTablesInCatalog(spark, batch_id)\n",
    "  \n",
    "  // Get file counts for each table\n",
    "  val tablesWithFileCounts = tables.map { table =>\n",
    "    val fileCount = getFileCount(spark, table)\n",
    "    (table, fileCount)\n",
    "  }\n",
    "\n",
    "  // Sort tables by file count (ascending order)\n",
    "  val sortedTables = tablesWithFileCounts.sortBy(_._2)\n",
    "\n",
    "  // Process tables from smallest to largest\n",
    "  sortedTables.foreach { case (table, _) =>\n",
    "    performDeletes(spark, table, dry_run, batch_id)\n",
    "  }\n",
    "    \n",
    "  log_to_tabular(batch_id, s\"decorruption.finish\", s\"\\n‚úÖ Completed batch $batch_id\")\n",
    "}\n",
    "\n",
    "def deduplicate_catalog(spark: SparkSession, dry_run: Boolean = true): Unit = {\n",
    "  val current_catalog = spark.catalog.currentCatalog\n",
    "  val batch_id = getRandomBatchId() + s\" -- $current_catalog\"\n",
    "  log_to_tabular(batch_id, s\"deduplicate_catalog.start\", s\"Started batch $batch_id\")\n",
    "    \n",
    "  // List all tables in the catalog\n",
    "  val tables = listTablesInCatalog(spark, batch_id)\n",
    "    \n",
    "  // build deduplicated versions of each table\n",
    "  tables.foreach { table =>\n",
    "    try {\n",
    "      deduplicate_table(spark, table, dry_run, batch_id)\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        // The typical exceptions have full stack traces in the error message, so grab just the first line of the message\n",
    "        log_to_tabular(batch_id, s\"deduplicate_table.error\", s\"‚ùå Error during deduplication for table $table: ${e.getMessage.split(\"\\n\").head}\")\n",
    "    }\n",
    "  }  \n",
    "    \n",
    "  log_to_tabular(batch_id, s\"deduplicate_catalog.finish\", s\"\\n‚úÖ Completed batch $batch_id\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18664dea",
   "metadata": {},
   "source": [
    "## Now, let's actually do the work üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorrupt_catalog(spark, false)\n",
    "// deduplicate_catalog(spark, false)\n",
    "// archive_and_promote_catalog(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
